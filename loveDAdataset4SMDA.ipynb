{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMDADataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Copyright (c) 2021-2022 ETH Zurich, Lukas Hoyer. All rights reserved.\n",
    "# Licensed under the Apache License, Version 2.0\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "import json\n",
    "import os.path as osp\n",
    "\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from . import CityscapesDataset\n",
    "from .builder import DATASETS\n",
    "\n",
    "\n",
    "def get_rcs_class_probs(data_root, temperature):\n",
    "    with open(osp.join(data_root, 'sample_class_stats.json'), 'r') as of:\n",
    "        sample_class_stats = json.load(of)\n",
    "    overall_class_stats = {}\n",
    "    for s in sample_class_stats:\n",
    "        s.pop('file')\n",
    "        for c, n in s.items():\n",
    "            c = int(c)\n",
    "            if c not in overall_class_stats:\n",
    "                overall_class_stats[c] = n\n",
    "            else:\n",
    "                overall_class_stats[c] += n\n",
    "    overall_class_stats = {\n",
    "        k: v\n",
    "        for k, v in sorted(\n",
    "            overall_class_stats.items(), key=lambda item: item[1])\n",
    "    }\n",
    "    freq = torch.tensor(list(overall_class_stats.values()))\n",
    "    freq = freq / torch.sum(freq)\n",
    "    freq = 1 - freq\n",
    "    freq = torch.softmax(freq / temperature, dim=-1)\n",
    "\n",
    "    return list(overall_class_stats.keys()), freq.numpy()\n",
    "\n",
    "\n",
    "@DATASETS.register_module()\n",
    "class SMDADataset(object):\n",
    "\n",
    "    def __init__(self, source, target, cfg):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.ignore_index = target.ignore_index\n",
    "        self.CLASSES = target.CLASSES\n",
    "        self.PALETTE = target.PALETTE\n",
    "        assert target.ignore_index == source.ignore_index\n",
    "        assert target.CLASSES == source.CLASSES\n",
    "        assert target.PALETTE == source.PALETTE\n",
    "\n",
    "        rcs_cfg = cfg.get('rare_class_sampling')\n",
    "        self.rcs_enabled = rcs_cfg is not None\n",
    "        if self.rcs_enabled:\n",
    "            self.rcs_class_temp = rcs_cfg['class_temp']\n",
    "            self.rcs_min_crop_ratio = rcs_cfg['min_crop_ratio']\n",
    "            self.rcs_min_pixels = rcs_cfg['min_pixels']\n",
    "\n",
    "            self.rcs_classes, self.rcs_classprob = get_rcs_class_probs(\n",
    "                cfg['source']['data_root'], self.rcs_class_temp)\n",
    "            mmcv.print_log(f'RCS Classes: {self.rcs_classes}', 'mmseg')\n",
    "            mmcv.print_log(f'RCS ClassProb: {self.rcs_classprob}', 'mmseg')\n",
    "\n",
    "            with open(\n",
    "                    osp.join(cfg['source']['data_root'],\n",
    "                             'samples_with_class.json'), 'r') as of:\n",
    "                samples_with_class_and_n = json.load(of)\n",
    "            samples_with_class_and_n = {\n",
    "                int(k): v\n",
    "                for k, v in samples_with_class_and_n.items()\n",
    "                if int(k) in self.rcs_classes\n",
    "            }\n",
    "            self.samples_with_class = {}\n",
    "            for c in self.rcs_classes:\n",
    "                self.samples_with_class[c] = []\n",
    "                for file, pixels in samples_with_class_and_n[c]:\n",
    "                    if pixels > self.rcs_min_pixels:\n",
    "                        self.samples_with_class[c].append(file.split('/')[-1])\n",
    "                assert len(self.samples_with_class[c]) > 0\n",
    "            self.file_to_idx = {}\n",
    "            for i, dic in enumerate(self.source.img_infos):\n",
    "                file = dic['ann']['seg_map']\n",
    "                if isinstance(self.source, CityscapesDataset):\n",
    "                    file = file.split('/')[-1]\n",
    "                self.file_to_idx[file] = i\n",
    "\n",
    "    def get_rare_class_sample(self):\n",
    "        c = np.random.choice(self.rcs_classes, p=self.rcs_classprob)\n",
    "        f1 = np.random.choice(self.samples_with_class[c])\n",
    "        i1 = self.file_to_idx[f1]\n",
    "        s1 = self.source[i1]\n",
    "        if self.rcs_min_crop_ratio > 0:\n",
    "            for j in range(10):\n",
    "                n_class = torch.sum(s1['gt_semantic_seg'].data == c)\n",
    "                # mmcv.print_log(f'{j}: {n_class}', 'mmseg')\n",
    "                if n_class > self.rcs_min_pixels * self.rcs_min_crop_ratio:\n",
    "                    break\n",
    "                # Sample a new random crop from source image i1.\n",
    "                # Please note, that self.source.__getitem__(idx) applies the\n",
    "                # preprocessing pipeline to the loaded image, which includes\n",
    "                # RandomCrop, and results in a new crop of the image.\n",
    "                s1 = self.source[i1]\n",
    "        i2 = np.random.choice(range(len(self.target)))\n",
    "        s2 = self.target[i2]\n",
    "        return {\n",
    "            **s1, 'target_img_metas': s2['img_metas'],\n",
    "            'target_img': (s2['img']), \n",
    "            'target_gt_semantic_seg': s2['gt_semantic_seg'], \n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.rcs_enabled:\n",
    "            return self.get_rare_class_sample()\n",
    "        else:\n",
    "            s1 = self.source[idx // len(self.target)]\n",
    "            s2 = self.target[idx % len(self.target)]\n",
    "            return {\n",
    "                **s1, 'target_img_metas': s2['img_metas'],\n",
    "                'target_img': s2['img'], \n",
    "                'target_gt_semantic_seg': s2['gt_semantic_seg'], \n",
    "            }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source) * len(self.target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoveDADataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "\n",
    "# Modifications add SMLoveDADataset \n",
    "import os.path as osp\n",
    "\n",
    "import mmcv\n",
    "from mmcv.utils import print_log\n",
    "from mmseg.utils import get_root_logger\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from .builder import DATASETS\n",
    "from .custom import CustomDataset\n",
    "\n",
    "\n",
    "@DATASETS.register_module()\n",
    "class LoveDADataset(CustomDataset):\n",
    "    \"\"\"LoveDA dataset.\n",
    "\n",
    "    In segmentation map annotation for LoveDA, 0 is the ignore index.\n",
    "    ``reduce_zero_label`` should be set to True. The ``img_suffix`` and\n",
    "    ``seg_map_suffix`` are both fixed to '.png'.\n",
    "    \"\"\"\n",
    "    CLASSES = ('background', 'building', 'road', 'water', 'barren', 'forest',\n",
    "               'agricultural')\n",
    "\n",
    "    PALETTE = [[255, 255, 255], [255, 0, 0], [255, 255, 0], [0, 0, 255],\n",
    "               [159, 129, 183], [0, 255, 0], [255, 195, 128]]\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LoveDADataset, self).__init__(\n",
    "            img_suffix='.png',\n",
    "            seg_map_suffix='.png',\n",
    "            reduce_zero_label=True,\n",
    "            **kwargs)\n",
    "\n",
    "    def results2img(self, results, imgfile_prefix, indices=None):\n",
    "        \"\"\"Write the segmentation results to images.\n",
    "\n",
    "        Args:\n",
    "            results (list[ndarray]): Testing results of the\n",
    "                dataset.\n",
    "            imgfile_prefix (str): The filename prefix of the png files.\n",
    "                If the prefix is \"somepath/xxx\",\n",
    "                the png files will be named \"somepath/xxx.png\".\n",
    "            indices (list[int], optional): Indices of input results, if not\n",
    "                set, all the indices of the dataset will be used.\n",
    "                Default: None.\n",
    "\n",
    "        Returns:\n",
    "            list[str: str]: result txt files which contains corresponding\n",
    "            semantic segmentation images.\n",
    "        \"\"\"\n",
    "\n",
    "        mmcv.mkdir_or_exist(imgfile_prefix)\n",
    "        result_files = []\n",
    "        for result, idx in zip(results, indices):\n",
    "\n",
    "            filename = self.img_infos[idx]['filename']\n",
    "            basename = osp.splitext(osp.basename(filename))[0]\n",
    "\n",
    "            png_filename = osp.join(imgfile_prefix, f'{basename}.png')\n",
    "\n",
    "            # The  index range of official requirement is from 0 to 6.\n",
    "            output = Image.fromarray(result.astype(np.uint8))\n",
    "            output.save(png_filename)\n",
    "            result_files.append(png_filename)\n",
    "\n",
    "        return result_files\n",
    "\n",
    "    def format_results(self, results, imgfile_prefix, indices=None):\n",
    "        \"\"\"Format the results into dir (standard format for LoveDA evaluation).\n",
    "\n",
    "        Args:\n",
    "            results (list): Testing results of the dataset.\n",
    "            imgfile_prefix (str): The prefix of images files. It\n",
    "                includes the file path and the prefix of filename, e.g.,\n",
    "                \"a/b/prefix\".\n",
    "            indices (list[int], optional): Indices of input results,\n",
    "                if not set, all the indices of the dataset will be used.\n",
    "                Default: None.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (result_files, tmp_dir), result_files is a list containing\n",
    "                the image paths, tmp_dir is the temporal directory created\n",
    "                for saving json/png files when img_prefix is not specified.\n",
    "        \"\"\"\n",
    "        if indices is None:\n",
    "            indices = list(range(len(self)))\n",
    "\n",
    "        assert isinstance(results, list), 'results must be a list.'\n",
    "        assert isinstance(indices, list), 'indices must be a list.'\n",
    "\n",
    "        result_files = self.results2img(results, imgfile_prefix, indices)\n",
    "\n",
    "        return result_files\n",
    "\n",
    "def get_len_ann(infos):\n",
    "    len_ann = []\n",
    "    for info in infos:\n",
    "        with_labels=info[\"ann\"].get('with_labels', False)\n",
    "        # print_log(ann, logger=get_root_logger())\n",
    "        if not with_labels:\n",
    "            continue\n",
    "        len_ann.append(with_labels)\n",
    "    return len(len_ann)\n",
    "@DATASETS.register_module()\n",
    "class SMLoveDADataset(LoveDADataset):\n",
    "    \"\"\" SMLoveDA dataset.\n",
    "    In segmentation map annotation for LoveDA, 0 is the ignore index.\n",
    "    ``reduce_zero_label`` should be set to True. The ``img_suffix`` and\n",
    "    ``seg_map_suffix`` are both fixed to '.png'.\n",
    "    \"\"\"\n",
    "    CLASSES = ('background', 'building', 'road', 'water', 'barren', 'forest',\n",
    "               'agricultural')\n",
    "\n",
    "    PALETTE = [[255, 255, 255], [255, 0, 0], [255, 255, 0], [0, 0, 255],\n",
    "               [159, 129, 183], [0, 255, 0], [255, 195, 128]]\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LoveDADataset, self).__init__(\n",
    "            img_suffix='.png',\n",
    "            seg_map_suffix='.png',\n",
    "            reduce_zero_label=True,\n",
    "            **kwargs)\n",
    "\n",
    "    def results2img(self, results, imgfile_prefix, indices=None):\n",
    "        \"\"\"Write the segmentation results to images.\n",
    "\n",
    "        Args:\n",
    "            results (list[ndarray]): Testing results of the\n",
    "                dataset.\n",
    "            imgfile_prefix (str): The filename prefix of the png files.\n",
    "                If the prefix is \"somepath/xxx\",\n",
    "                the png files will be named \"somepath/xxx.png\".\n",
    "            indices (list[int], optional): Indices of input results, if not\n",
    "                set, all the indices of the dataset will be used.\n",
    "                Default: None.\n",
    "\n",
    "        Returns:\n",
    "            list[str: str]: result txt files which contains corresponding\n",
    "            semantic segmentation images.\n",
    "        \"\"\"\n",
    "\n",
    "        mmcv.mkdir_or_exist(imgfile_prefix)\n",
    "        result_files = []\n",
    "        for result, idx in zip(results, indices):\n",
    "\n",
    "            filename = self.img_infos[idx]['filename']\n",
    "            basename = osp.splitext(osp.basename(filename))[0]\n",
    "\n",
    "            png_filename = osp.join(imgfile_prefix, f'{basename}.png')\n",
    "\n",
    "            # The  index range of official requirement is from 0 to 6.\n",
    "            output = Image.fromarray(result.astype(np.uint8))\n",
    "            output.save(png_filename)\n",
    "            result_files.append(png_filename)\n",
    "\n",
    "        return result_files\n",
    "\n",
    "    def format_results(self, results, imgfile_prefix, indices=None):\n",
    "        \"\"\"Format the results into dir (standard format for LoveDA evaluation).\n",
    "\n",
    "        Args:\n",
    "            results (list): Testing results of the dataset.\n",
    "            imgfile_prefix (str): The prefix of images files. It\n",
    "                includes the file path and the prefix of filename, e.g.,\n",
    "                \"a/b/prefix\".\n",
    "            indices (list[int], optional): Indices of input results,\n",
    "                if not set, all the indices of the dataset will be used.\n",
    "                Default: None.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (result_files, tmp_dir), result_files is a list containing\n",
    "                the image paths, tmp_dir is the temporal directory created\n",
    "                for saving json/png files when img_prefix is not specified.\n",
    "        \"\"\"\n",
    "        if indices is None:\n",
    "            indices = list(range(len(self)))\n",
    "\n",
    "        assert isinstance(results, list), 'results must be a list.'\n",
    "        assert isinstance(indices, list), 'indices must be a list.'\n",
    "\n",
    "        result_files = self.results2img(results, imgfile_prefix, indices)\n",
    "\n",
    "        return result_files\n",
    "    def load_annotations(self, img_dir, img_suffix, ann_dir, seg_map_suffix,\n",
    "                         split):\n",
    "        \"\"\"Load annotation from directory.\n",
    "        If split is specificed, file with suffix in the splits will be loaded with labels\n",
    "        otherwise images will be loaded without labels.\n",
    "\n",
    "        Args:\n",
    "            img_dir (str): Path to image directory\n",
    "            img_suffix (str): Suffix of images.\n",
    "            ann_dir (str|None): Path to annotation directory.\n",
    "            seg_map_suffix (str|None): Suffix of segmentation maps.\n",
    "            split (str|None): Split txt file. If split is specified, only file\n",
    "                with suffix in the splits will be loaded. Otherwise, all images\n",
    "                in img_dir/ann_dir will be loaded. Default: None\n",
    "\n",
    "        Returns:\n",
    "            list[dict]: All image info of dataset.\n",
    "        \"\"\"\n",
    "        with open(split) as f:\n",
    "            split=[(line.strip()) for line in f]\n",
    "        # print_log(split, logger=get_root_logger())\n",
    "        img_infos = []\n",
    "\n",
    "        for img in mmcv.scandir(img_dir, img_suffix, recursive=True):\n",
    "            img_info = dict(filename=img)\n",
    "            if img.split('/')[-1].split('.')[0] in split and ann_dir is not None:\n",
    "                img_info['ann'] = dict(\n",
    "                    seg_map=img.replace(img_suffix, seg_map_suffix),\n",
    "                    with_labels=True\n",
    "                    )\n",
    "                # print_log(f\"split in {img_info['ann']} \", logger=get_root_logger())                 \n",
    "            else:\n",
    "                img_info['ann'] = dict(\n",
    "                    seg_map=img.replace(img_suffix, seg_map_suffix),\n",
    "                    with_labels=False)\n",
    "                # print_log(f\"split not in {img_info['ann']} \", logger=get_root_logger()) \n",
    "\n",
    "            img_infos.append(img_info)\n",
    "                    \n",
    "        print_log(\n",
    "            f\"Loaded {len(img_infos)} images from {img_dir}, {get_len_ann(img_infos)} images with labels\",\n",
    "            logger=get_root_logger())\n",
    "        # print_log(\n",
    "        #     img_infos, logger=get_root_logger()\n",
    "        # )\n",
    "        return img_infos\n",
    "    def get_ann_info(self, idx):\n",
    "        \"\"\"Get annotation by index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of data.\n",
    "\n",
    "        Returns:\n",
    "            dict: Annotation info of specified index.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.img_infos[idx]['ann']\n",
    "    def prepare_train_img(self, idx):\n",
    "        \"\"\"Get training data and annotations after pipeline.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of data.\n",
    "\n",
    "        Returns:\n",
    "            dict: Training data and annotation after pipeline with new keys\n",
    "                introduced by pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        img_info = self.img_infos[idx]\n",
    "        ann_info = self.get_ann_info(idx)\n",
    "        img_info['with_labels'] = ann_info.get('with_labels', False)\n",
    "        # print_log(img_info, logger=get_root_logger())\n",
    "\n",
    "        results = dict(img_info=img_info, ann_info=ann_info)\n",
    "        self.pre_pipeline(results)\n",
    "        return self.pipeline(results)\n",
    "\n",
    "    def prepare_test_img(self, idx):\n",
    "        \"\"\"Get testing data after pipeline.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of data.\n",
    "\n",
    "        Returns:\n",
    "            dict: Testing data after pipeline with new keys introduced by\n",
    "                pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        img_info = self.img_infos[idx]\n",
    "        ann_info = self.get_ann_info(idx)\n",
    "        img_info['with_labels'] = ann_info.get('with_labels', False)\n",
    "        print_log(img_info, logger=get_root_logger())\n",
    "\n",
    "        results = dict(img_info=img_info)\n",
    "        self.pre_pipeline(results)\n",
    "        return self.pipeline(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained from: https://github.com/open-mmlab/mmsegmentation/tree/v0.16.0\n",
    "\n",
    "import os.path as osp\n",
    "\n",
    "import mmcv\n",
    "import numpy as np\n",
    "from mmcv.utils import print_log\n",
    "from mmseg.utils import get_root_logger\n",
    "from ..builder import PIPELINES\n",
    "\n",
    "\n",
    "@PIPELINES.register_module()\n",
    "class LoadImageFromFile(object):\n",
    "    \"\"\"Load an image from file.\n",
    "\n",
    "    Required keys are \"img_prefix\" and \"img_info\" (a dict that must contain the\n",
    "    key \"filename\"). Added or updated keys are \"filename\", \"img\", \"img_shape\",\n",
    "    \"ori_shape\" (same as `img_shape`), \"pad_shape\" (same as `img_shape`),\n",
    "    \"scale_factor\" (1.0) and \"img_norm_cfg\" (means=0 and stds=1).\n",
    "\n",
    "    Args:\n",
    "        to_float32 (bool): Whether to convert the loaded image to a float32\n",
    "            numpy array. If set to False, the loaded image is an uint8 array.\n",
    "            Defaults to False.\n",
    "        color_type (str): The flag argument for :func:`mmcv.imfrombytes`.\n",
    "            Defaults to 'color'.\n",
    "        file_client_args (dict): Arguments to instantiate a FileClient.\n",
    "            See :class:`mmcv.fileio.FileClient` for details.\n",
    "            Defaults to ``dict(backend='disk')``.\n",
    "        imdecode_backend (str): Backend for :func:`mmcv.imdecode`. Default:\n",
    "            'cv2'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 to_float32=False,\n",
    "                 color_type='color',\n",
    "                 file_client_args=dict(backend='disk'),\n",
    "                 imdecode_backend='cv2'):\n",
    "        self.to_float32 = to_float32\n",
    "        self.color_type = color_type\n",
    "        self.file_client_args = file_client_args.copy()\n",
    "        self.file_client = None\n",
    "        self.imdecode_backend = imdecode_backend\n",
    "\n",
    "    def __call__(self, results):\n",
    "        \"\"\"Call functions to load image and get image meta information.\n",
    "\n",
    "        Args:\n",
    "            results (dict): Result dict from :obj:`mmseg.CustomDataset`.\n",
    "\n",
    "        Returns:\n",
    "            dict: The dict contains loaded image and meta information.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.file_client is None:\n",
    "            self.file_client = mmcv.FileClient(**self.file_client_args)\n",
    "\n",
    "        if results.get('img_prefix') is not None:\n",
    "            filename = osp.join(results['img_prefix'],\n",
    "                                results['img_info']['filename'])\n",
    "        else:\n",
    "            filename = results['img_info']['filename']\n",
    "        img_bytes = self.file_client.get(filename)\n",
    "        img = mmcv.imfrombytes(\n",
    "            img_bytes, flag=self.color_type, backend=self.imdecode_backend)\n",
    "        if self.to_float32:\n",
    "            img = img.astype(np.float32)\n",
    "        results['filename'] = filename\n",
    "        results['ori_filename'] = results['img_info']['filename']\n",
    "        results['with_labels']  = results['img_info'].get('with_labels', False)\n",
    "\n",
    "        # print_log(results, logger=get_root_logger())\n",
    "        results['img'] = img\n",
    "        results['img_shape'] = img.shape\n",
    "        results['ori_shape'] = img.shape\n",
    "        # Set initial values for default meta_keys\n",
    "        results['pad_shape'] = img.shape\n",
    "        results['scale_factor'] = 1.0\n",
    "        num_channels = 1 if len(img.shape) < 3 else img.shape[2]\n",
    "        results['img_norm_cfg'] = dict(\n",
    "            mean=np.zeros(num_channels, dtype=np.float32),\n",
    "            std=np.ones(num_channels, dtype=np.float32),\n",
    "            to_rgb=False)\n",
    "        return results\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr_str = self.__class__.__name__\n",
    "        repr_str += f'(to_float32={self.to_float32},'\n",
    "        repr_str += f\"color_type='{self.color_type}',\"\n",
    "        repr_str += f\"imdecode_backend='{self.imdecode_backend}')\"\n",
    "        return repr_str\n",
    "\n",
    "\n",
    "@PIPELINES.register_module()\n",
    "class LoadAnnotations(object):\n",
    "    \"\"\"Load annotations for semantic segmentation.\n",
    "\n",
    "    Args:\n",
    "        reduce_zero_label (bool): Whether reduce all label value by 1.\n",
    "            Usually used for datasets where 0 is background label.\n",
    "            Default: False.\n",
    "        file_client_args (dict): Arguments to instantiate a FileClient.\n",
    "            See :class:`mmcv.fileio.FileClient` for details.\n",
    "            Defaults to ``dict(backend='disk')``.\n",
    "        imdecode_backend (str): Backend for :func:`mmcv.imdecode`. Default:\n",
    "            'pillow'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 reduce_zero_label=False,\n",
    "                 file_client_args=dict(backend='disk'),\n",
    "                 imdecode_backend='pillow'):\n",
    "        self.reduce_zero_label = reduce_zero_label\n",
    "        self.file_client_args = file_client_args.copy()\n",
    "        self.file_client = None\n",
    "        self.imdecode_backend = imdecode_backend\n",
    "\n",
    "    def __call__(self, results):\n",
    "        \"\"\"Call function to load multiple types annotations.\n",
    "\n",
    "        Args:\n",
    "            results (dict): Result dict from :obj:`mmseg.CustomDataset`.\n",
    "\n",
    "        Returns:\n",
    "            dict: The dict contains loaded semantic segmentation annotations.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.file_client is None:\n",
    "            self.file_client = mmcv.FileClient(**self.file_client_args)\n",
    "        # print_log(results, logger=get_root_logger())\n",
    "\n",
    "        if results.get('seg_prefix', None) is not None:\n",
    "            filename = osp.join(results['seg_prefix'],\n",
    "                                results['ann_info']['seg_map'])\n",
    "            # results['use_gt'] = int(results['ann_info']['seg_map'].split('.')[0]) == 404            \n",
    "        else:\n",
    "            filename = results['ann_info']['seg_map']      \n",
    "        # results['with_labels']=results['img_info']['ann'].get('with_labels',False)\n",
    "        img_bytes = self.file_client.get(filename)\n",
    "        gt_semantic_seg = mmcv.imfrombytes(\n",
    "            img_bytes, flag='unchanged',\n",
    "            backend=self.imdecode_backend).squeeze().astype(np.uint8)\n",
    "        # modify if custom classes\n",
    "        if results.get('label_map', None) is not None:\n",
    "            for old_id, new_id in results['label_map'].items():\n",
    "                gt_semantic_seg[gt_semantic_seg == old_id] = new_id\n",
    "        # reduce zero_label\n",
    "        if self.reduce_zero_label:\n",
    "            # avoid using underflow conversion\n",
    "            gt_semantic_seg[gt_semantic_seg == 0] = 255\n",
    "            gt_semantic_seg = gt_semantic_seg - 1\n",
    "            gt_semantic_seg[gt_semantic_seg == 254] = 255\n",
    "        results['gt_semantic_seg'] = gt_semantic_seg\n",
    "        results['seg_fields'].append('gt_semantic_seg')\n",
    "        # print_log(results, logger=get_root_logger())\n",
    "        results['with_labels']  = results['ann_info'].get('with_labels', False)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr_str = self.__class__.__name__\n",
    "        repr_str += f'(reduce_zero_label={self.reduce_zero_label},'\n",
    "        repr_str += f\"imdecode_backend='{self.imdecode_backend}')\"\n",
    "        return repr_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@PIPELINES.register_module()\n",
    "class Collect(object):\n",
    "    \"\"\"Collect data from the loader relevant to the specific task.\n",
    "\n",
    "    This is usually the last stage of the data loader pipeline. Typically keys\n",
    "    is set to some subset of \"img\", \"gt_semantic_seg\".\n",
    "\n",
    "    The \"img_meta\" item is always populated.  The contents of the \"img_meta\"\n",
    "    dictionary depends on \"meta_keys\". By default this includes:\n",
    "\n",
    "        - \"img_shape\": shape of the image input to the network as a tuple\n",
    "            (h, w, c).  Note that images may be zero padded on the bottom/right\n",
    "            if the batch tensor is larger than this shape.\n",
    "\n",
    "        - \"scale_factor\": a float indicating the preprocessing scale\n",
    "\n",
    "        - \"flip\": a boolean indicating if image flip transform was used\n",
    "\n",
    "        - \"filename\": path to the image file\n",
    "\n",
    "        - \"ori_shape\": original shape of the image as a tuple (h, w, c)\n",
    "\n",
    "        - \"pad_shape\": image shape after padding\n",
    "\n",
    "        - \"img_norm_cfg\": a dict of normalization information:\n",
    "            - mean - per channel mean subtraction\n",
    "            - std - per channel std divisor\n",
    "            - to_rgb - bool indicating if bgr was converted to rgb\n",
    "\n",
    "    Args:\n",
    "        keys (Sequence[str]): Keys of results to be collected in ``data``.\n",
    "        meta_keys (Sequence[str], optional): Meta keys to be converted to\n",
    "            ``mmcv.DataContainer`` and collected in ``data[img_metas]``.\n",
    "            Default: ``('filename', 'ori_filename', 'ori_shape', 'img_shape',\n",
    "            'pad_shape', 'scale_factor', 'flip', 'flip_direction',\n",
    "            'img_norm_cfg')``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 keys,\n",
    "                 meta_keys=('filename', 'ori_filename', 'ori_shape',\n",
    "                            'img_shape', 'pad_shape', 'scale_factor', 'flip',\n",
    "                            'flip_direction', 'img_norm_cfg', 'with_labels')):\n",
    "        self.keys = keys\n",
    "        self.meta_keys = meta_keys\n",
    "\n",
    "    def __call__(self, results):\n",
    "        \"\"\"Call function to collect keys in results. The keys in ``meta_keys``\n",
    "        will be converted to :obj:mmcv.DataContainer.\n",
    "\n",
    "        Args:\n",
    "            results (dict): Result dict contains the data to collect.\n",
    "\n",
    "        Returns:\n",
    "            dict: The result dict contains the following keys\n",
    "                - keys in``self.keys``\n",
    "                - ``img_metas``\n",
    "        \"\"\"\n",
    "\n",
    "        data = {}\n",
    "        img_meta = {}\n",
    "        for key in self.meta_keys:\n",
    "            img_meta[key] = results[key]\n",
    "        data['img_metas'] = DC(img_meta, cpu_only=True)\n",
    "        for key in self.keys:\n",
    "            data[key] = results[key]\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "               f'(keys={self.keys}, meta_keys={self.meta_keys})'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DACS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Copyright (c) 2021-2022 ETH Zurich, Lukas Hoyer. All rights reserved.\n",
    "# Licensed under the Apache License, Version 2.0\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# The ema model update and the domain-mixing are based on:\n",
    "# https://github.com/vikolss/DACS\n",
    "# Copyright (c) 2020 vikolss. Licensed under the MIT License.\n",
    "# A copy of the license is available at resources/license_dacs\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from timm.models.layers import DropPath\n",
    "from torch.nn.modules.dropout import _DropoutNd\n",
    "\n",
    "from mmseg.core import add_prefix\n",
    "from mmseg.models import UDA, build_segmentor\n",
    "from mmseg.models.uda.uda_decorator import UDADecorator, get_module\n",
    "from mmseg.models.utils.dacs_transforms import (denorm, get_class_masks,\n",
    "                                                get_mean_std, strong_transform)\n",
    "from mmseg.models.utils.visualization import subplotimg\n",
    "from mmseg.utils.utils import downscale_label_ratio\n",
    "\n",
    "\n",
    "def _params_equal(ema_model, model):\n",
    "    for ema_param, param in zip(ema_model.named_parameters(),\n",
    "                                model.named_parameters()):\n",
    "        if not torch.equal(ema_param[1].data, param[1].data):\n",
    "            # print(\"Difference in\", ema_param[0])\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def calc_grad_magnitude(grads, norm_type=2.0):\n",
    "    norm_type = float(norm_type)\n",
    "    if norm_type == math.inf:\n",
    "        norm = max(p.abs().max() for p in grads)\n",
    "    else:\n",
    "        norm = torch.norm(\n",
    "            torch.stack([torch.norm(p, norm_type) for p in grads]), norm_type)\n",
    "\n",
    "    return norm\n",
    "\n",
    "\n",
    "@UDA.register_module()\n",
    "class DACS(UDADecorator):\n",
    "\n",
    "    def __init__(self, **cfg):\n",
    "        super(DACS, self).__init__(**cfg)\n",
    "        self.local_iter = 0\n",
    "        self.max_iters = cfg['max_iters']\n",
    "        self.alpha = cfg['alpha']\n",
    "        self.pseudo_threshold = cfg['pseudo_threshold']\n",
    "        self.psweight_ignore_top = cfg['pseudo_weight_ignore_top']\n",
    "        self.psweight_ignore_bottom = cfg['pseudo_weight_ignore_bottom']\n",
    "        self.fdist_lambda = cfg['imnet_feature_dist_lambda']\n",
    "        self.fdist_classes = cfg['imnet_feature_dist_classes']\n",
    "        self.fdist_scale_min_ratio = cfg['imnet_feature_dist_scale_min_ratio']\n",
    "        self.enable_fdist = self.fdist_lambda > 0\n",
    "        self.mix = cfg['mix']\n",
    "        self.blur = cfg['blur']\n",
    "        self.color_jitter_s = cfg['color_jitter_strength']\n",
    "        self.color_jitter_p = cfg['color_jitter_probability']\n",
    "        self.debug_img_interval = cfg['debug_img_interval']\n",
    "        self.print_grad_magnitude = cfg['print_grad_magnitude']\n",
    "        assert self.mix == 'class'\n",
    "\n",
    "        self.debug_fdist_mask = None\n",
    "        self.debug_gt_rescale = None\n",
    "\n",
    "        self.class_probs = {}\n",
    "        ema_cfg = deepcopy(cfg['model'])\n",
    "        self.ema_model = build_segmentor(ema_cfg)\n",
    "\n",
    "        if self.enable_fdist:\n",
    "            self.imnet_model = build_segmentor(deepcopy(cfg['model']))\n",
    "        else:\n",
    "            self.imnet_model = None\n",
    "\n",
    "    def get_ema_model(self):\n",
    "        return get_module(self.ema_model)\n",
    "\n",
    "    def get_imnet_model(self):\n",
    "        return get_module(self.imnet_model)\n",
    "\n",
    "    def _init_ema_weights(self):\n",
    "        for param in self.get_ema_model().parameters():\n",
    "            param.detach_()\n",
    "        mp = list(self.get_model().parameters())\n",
    "        mcp = list(self.get_ema_model().parameters())\n",
    "        for i in range(0, len(mp)):\n",
    "            if not mcp[i].data.shape:  # scalar tensor\n",
    "                mcp[i].data = mp[i].data.clone()\n",
    "            else:\n",
    "                mcp[i].data[:] = mp[i].data[:].clone()\n",
    "\n",
    "    def _update_ema(self, iter):\n",
    "        alpha_teacher = min(1 - 1 / (iter + 1), self.alpha)\n",
    "        for ema_param, param in zip(self.get_ema_model().parameters(),\n",
    "                                    self.get_model().parameters()):\n",
    "            if not param.data.shape:  # scalar tensor\n",
    "                ema_param.data = \\\n",
    "                    alpha_teacher * ema_param.data + \\\n",
    "                    (1 - alpha_teacher) * param.data\n",
    "            else:\n",
    "                ema_param.data[:] = \\\n",
    "                    alpha_teacher * ema_param[:].data[:] + \\\n",
    "                    (1 - alpha_teacher) * param[:].data[:]\n",
    "\n",
    "    def train_step(self, data_batch, optimizer, **kwargs):\n",
    "        \"\"\"The iteration step during training.\n",
    "\n",
    "        This method defines an iteration step during training, except for the\n",
    "        back propagation and optimizer updating, which are done in an optimizer\n",
    "        hook. Note that in some complicated cases or models, the whole process\n",
    "        including back propagation and optimizer updating is also defined in\n",
    "        this method, such as GAN.\n",
    "\n",
    "        Args:\n",
    "            data (dict): The output of dataloader.\n",
    "            optimizer (:obj:`torch.optim.Optimizer` | dict): The optimizer of\n",
    "                runner is passed to ``train_step()``. This argument is unused\n",
    "                and reserved.\n",
    "\n",
    "        Returns:\n",
    "            dict: It should contain at least 3 keys: ``loss``, ``log_vars``,\n",
    "                ``num_samples``.\n",
    "                ``loss`` is a tensor for back propagation, which can be a\n",
    "                weighted sum of multiple losses.\n",
    "                ``log_vars`` contains all the variables to be sent to the\n",
    "                logger.\n",
    "                ``num_samples`` indicates the batch size (when the model is\n",
    "                DDP, it means the batch size on each GPU), which is used for\n",
    "                averaging the logs.\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        log_vars = self(**data_batch)\n",
    "        optimizer.step()\n",
    "\n",
    "        log_vars.pop('loss', None)  # remove the unnecessary 'loss'\n",
    "        outputs = dict(\n",
    "            log_vars=log_vars, num_samples=len(data_batch['img_metas']))\n",
    "        return outputs\n",
    "\n",
    "    def masked_feat_dist(self, f1, f2, mask=None):\n",
    "        feat_diff = f1 - f2\n",
    "        # mmcv.print_log(f'fdiff: {feat_diff.shape}', 'mmseg')\n",
    "        pw_feat_dist = torch.norm(feat_diff, dim=1, p=2)\n",
    "        # mmcv.print_log(f'pw_fdist: {pw_feat_dist.shape}', 'mmseg')\n",
    "        if mask is not None:\n",
    "            # mmcv.print_log(f'fd mask: {mask.shape}', 'mmseg')\n",
    "            pw_feat_dist = pw_feat_dist[mask.squeeze(1)]\n",
    "            # mmcv.print_log(f'fd masked: {pw_feat_dist.shape}', 'mmseg')\n",
    "        return torch.mean(pw_feat_dist)\n",
    "\n",
    "    def calc_feat_dist(self, img, gt, feat=None):\n",
    "        assert self.enable_fdist\n",
    "        with torch.no_grad():\n",
    "            self.get_imnet_model().eval()\n",
    "            feat_imnet = self.get_imnet_model().extract_feat(img)\n",
    "            feat_imnet = [f.detach() for f in feat_imnet]\n",
    "        lay = -1\n",
    "        if self.fdist_classes is not None:\n",
    "            fdclasses = torch.tensor(self.fdist_classes, device=gt.device)\n",
    "            scale_factor = gt.shape[-1] // feat[lay].shape[-1]\n",
    "            gt_rescaled = downscale_label_ratio(gt, scale_factor,\n",
    "                                                self.fdist_scale_min_ratio,\n",
    "                                                self.num_classes,\n",
    "                                                255).long().detach()\n",
    "            fdist_mask = torch.any(gt_rescaled[..., None] == fdclasses, -1)\n",
    "            feat_dist = self.masked_feat_dist(feat[lay], feat_imnet[lay],\n",
    "                                              fdist_mask)\n",
    "            self.debug_fdist_mask = fdist_mask\n",
    "            self.debug_gt_rescale = gt_rescaled\n",
    "        else:\n",
    "            feat_dist = self.masked_feat_dist(feat[lay], feat_imnet[lay])\n",
    "        feat_dist = self.fdist_lambda * feat_dist\n",
    "        feat_loss, feat_log = self._parse_losses(\n",
    "            {'loss_imnet_feat_dist': feat_dist})\n",
    "        feat_log.pop('loss', None)\n",
    "        return feat_loss, feat_log\n",
    "\n",
    "    def forward_train(self, img, img_metas, gt_semantic_seg, target_img,\n",
    "                      target_img_metas):\n",
    "        \"\"\"Forward function for training.\n",
    "\n",
    "        Args:\n",
    "            img (Tensor): Input images.\n",
    "            img_metas (list[dict]): List of image info dict where each dict\n",
    "                has: 'img_shape', 'scale_factor', 'flip', and may also contain\n",
    "                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n",
    "                For details on the values of these keys see\n",
    "                `mmseg/datasets/pipelines/formatting.py:Collect`.\n",
    "            gt_semantic_seg (Tensor): Semantic segmentation masks\n",
    "                used if the architecture supports semantic segmentation task.\n",
    "\n",
    "        Returns:\n",
    "            dict[str, Tensor]: a dictionary of loss components\n",
    "        \"\"\"\n",
    "        log_vars = {}\n",
    "        batch_size = img.shape[0]\n",
    "        dev = img.device\n",
    "\n",
    "        # Init/update ema model\n",
    "        if self.local_iter == 0:\n",
    "            self._init_ema_weights()\n",
    "            # assert _params_equal(self.get_ema_model(), self.get_model())\n",
    "\n",
    "        if self.local_iter > 0:\n",
    "            self._update_ema(self.local_iter)\n",
    "            # assert not _params_equal(self.get_ema_model(), self.get_model())\n",
    "            # assert self.get_ema_model().training\n",
    "\n",
    "        means, stds = get_mean_std(img_metas, dev)\n",
    "        strong_parameters = {\n",
    "            'mix': None,\n",
    "            'color_jitter': random.uniform(0, 1),\n",
    "            'color_jitter_s': self.color_jitter_s,\n",
    "            'color_jitter_p': self.color_jitter_p,\n",
    "            'blur': random.uniform(0, 1) if self.blur else 0,\n",
    "            'mean': means[0].unsqueeze(0),  # assume same normalization\n",
    "            'std': stds[0].unsqueeze(0)\n",
    "        }\n",
    "\n",
    "        # Train on source images\n",
    "        clean_losses = self.get_model().forward_train(\n",
    "            img, img_metas, gt_semantic_seg, return_feat=True)\n",
    "        src_feat = clean_losses.pop('features')\n",
    "        clean_loss, clean_log_vars = self._parse_losses(clean_losses)\n",
    "        log_vars.update(clean_log_vars)\n",
    "        clean_loss.backward(retain_graph=self.enable_fdist)\n",
    "        if self.print_grad_magnitude:\n",
    "            params = self.get_model().backbone.parameters()\n",
    "            seg_grads = [\n",
    "                p.grad.detach().clone() for p in params if p.grad is not None\n",
    "            ]\n",
    "            grad_mag = calc_grad_magnitude(seg_grads)\n",
    "            mmcv.print_log(f'Seg. Grad.: {grad_mag}', 'mmseg')\n",
    "\n",
    "        # ImageNet feature distance\n",
    "        if self.enable_fdist:\n",
    "            feat_loss, feat_log = self.calc_feat_dist(img, gt_semantic_seg,\n",
    "                                                      src_feat)\n",
    "            feat_loss.backward()\n",
    "            log_vars.update(add_prefix(feat_log, 'src'))\n",
    "            if self.print_grad_magnitude:\n",
    "                params = self.get_model().backbone.parameters()\n",
    "                fd_grads = [\n",
    "                    p.grad.detach() for p in params if p.grad is not None\n",
    "                ]\n",
    "                fd_grads = [g2 - g1 for g1, g2 in zip(seg_grads, fd_grads)]\n",
    "                grad_mag = calc_grad_magnitude(fd_grads)\n",
    "                mmcv.print_log(f'Fdist Grad.: {grad_mag}', 'mmseg')\n",
    "\n",
    "        # Generate pseudo-label\n",
    "        for m in self.get_ema_model().modules():\n",
    "            if isinstance(m, _DropoutNd):\n",
    "                m.training = False\n",
    "            if isinstance(m, DropPath):\n",
    "                m.training = False\n",
    "        ema_logits = self.get_ema_model().encode_decode(\n",
    "            target_img, target_img_metas)\n",
    "\n",
    "        ema_softmax = torch.softmax(ema_logits.detach(), dim=1)\n",
    "        pseudo_prob, pseudo_label = torch.max(ema_softmax, dim=1)\n",
    "        ps_large_p = pseudo_prob.ge(self.pseudo_threshold).long() == 1\n",
    "        ps_size = np.size(np.array(pseudo_label.cpu()))\n",
    "        pseudo_weight = torch.sum(ps_large_p).item() / ps_size\n",
    "        pseudo_weight = pseudo_weight * torch.ones(\n",
    "            pseudo_prob.shape, device=dev)\n",
    "\n",
    "        if self.psweight_ignore_top > 0:\n",
    "            # Don't trust pseudo-labels in regions with potential\n",
    "            # rectification artifacts. This can lead to a pseudo-label\n",
    "            # drift from sky towards building or traffic light.\n",
    "            pseudo_weight[:, :self.psweight_ignore_top, :] = 0\n",
    "        if self.psweight_ignore_bottom > 0:\n",
    "            pseudo_weight[:, -self.psweight_ignore_bottom:, :] = 0\n",
    "        gt_pixel_weight = torch.ones((pseudo_weight.shape), device=dev)\n",
    "\n",
    "        # Apply mixing\n",
    "        mixed_img, mixed_lbl = [None] * batch_size, [None] * batch_size\n",
    "        mix_masks = get_class_masks(gt_semantic_seg)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            strong_parameters['mix'] = mix_masks[i]\n",
    "            mixed_img[i], mixed_lbl[i] = strong_transform(\n",
    "                strong_parameters,\n",
    "                data=torch.stack((img[i], target_img[i])),\n",
    "                target=torch.stack((gt_semantic_seg[i][0], pseudo_label[i])))\n",
    "            _, pseudo_weight[i] = strong_transform(\n",
    "                strong_parameters,\n",
    "                target=torch.stack((gt_pixel_weight[i], pseudo_weight[i])))\n",
    "        mixed_img = torch.cat(mixed_img)\n",
    "        mixed_lbl = torch.cat(mixed_lbl)\n",
    "\n",
    "        # Train on mixed images\n",
    "        mix_losses = self.get_model().forward_train(\n",
    "            mixed_img, img_metas, mixed_lbl, pseudo_weight, return_feat=True)\n",
    "        mix_losses.pop('features')\n",
    "        mix_losses = add_prefix(mix_losses, 'mix')\n",
    "        mix_loss, mix_log_vars = self._parse_losses(mix_losses)\n",
    "        log_vars.update(mix_log_vars)\n",
    "        mix_loss.backward()\n",
    "\n",
    "        if self.local_iter % self.debug_img_interval == 0:\n",
    "            out_dir = os.path.join(self.train_cfg['work_dir'],\n",
    "                                   'class_mix_debug')\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            vis_img = torch.clamp(denorm(img, means, stds), 0, 1)\n",
    "            vis_trg_img = torch.clamp(denorm(target_img, means, stds), 0, 1)\n",
    "            vis_mixed_img = torch.clamp(denorm(mixed_img, means, stds), 0, 1)\n",
    "            for j in range(batch_size):\n",
    "                rows, cols = 2, 5\n",
    "                fig, axs = plt.subplots(\n",
    "                    rows,\n",
    "                    cols,\n",
    "                    figsize=(3 * cols, 3 * rows),\n",
    "                    gridspec_kw={\n",
    "                        'hspace': 0.1,\n",
    "                        'wspace': 0,\n",
    "                        'top': 0.95,\n",
    "                        'bottom': 0,\n",
    "                        'right': 1,\n",
    "                        'left': 0\n",
    "                    },\n",
    "                )\n",
    "                subplotimg(axs[0][0], vis_img[j], 'Source Image')\n",
    "                subplotimg(axs[1][0], vis_trg_img[j], 'Target Image')\n",
    "                subplotimg(\n",
    "                    axs[0][1],\n",
    "                    gt_semantic_seg[j],\n",
    "                    'Source Seg GT',\n",
    "                    cmap='cityscapes')\n",
    "                subplotimg(\n",
    "                    axs[1][1],\n",
    "                    pseudo_label[j],\n",
    "                    'Target Seg (Pseudo) GT',\n",
    "                    cmap='cityscapes')\n",
    "                subplotimg(axs[0][2], vis_mixed_img[j], 'Mixed Image')\n",
    "                subplotimg(\n",
    "                    axs[1][2], mix_masks[j][0], 'Domain Mask', cmap='gray')\n",
    "                # subplotimg(axs[0][3], pred_u_s[j], \"Seg Pred\",\n",
    "                #            cmap=\"cityscapes\")\n",
    "                subplotimg(\n",
    "                    axs[1][3], mixed_lbl[j], 'Seg Targ', cmap='cityscapes')\n",
    "                subplotimg(\n",
    "                    axs[0][3], pseudo_weight[j], 'Pseudo W.', vmin=0, vmax=1)\n",
    "                if self.debug_fdist_mask is not None:\n",
    "                    subplotimg(\n",
    "                        axs[0][4],\n",
    "                        self.debug_fdist_mask[j][0],\n",
    "                        'FDist Mask',\n",
    "                        cmap='gray')\n",
    "                if self.debug_gt_rescale is not None:\n",
    "                    subplotimg(\n",
    "                        axs[1][4],\n",
    "                        self.debug_gt_rescale[j],\n",
    "                        'Scaled GT',\n",
    "                        cmap='cityscapes')\n",
    "                for ax in axs.flat:\n",
    "                    ax.axis('off')\n",
    "                plt.savefig(\n",
    "                    os.path.join(out_dir,\n",
    "                                 f'{(self.local_iter + 1):06d}_{j}.png'))\n",
    "                plt.close()\n",
    "        self.local_iter += 1\n",
    "\n",
    "        return log_vars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMDACs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Copyright (c) 2021-2022 ETH Zurich, Lukas Hoyer. All rights reserved.\n",
    "# Licensed under the Apache License, Version 2.0\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# The ema model update and the domain-mixing are based on:\n",
    "# https://github.com/vikolss/DACS\n",
    "# Copyright (c) 2020 vikolss. Licensed under the MIT License.\n",
    "# A copy of the license is available at resources/license_dacs\n",
    "\n",
    "# Modification: \n",
    "# - Support for loveda cmap\n",
    "# - add target_gt\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from timm.models.layers import DropPath\n",
    "from torch.nn.modules.dropout import _DropoutNd\n",
    "\n",
    "from mmseg.core import add_prefix\n",
    "from mmseg.models import UDA, build_segmentor\n",
    "from mmseg.models.uda.uda_decorator import UDADecorator, get_module\n",
    "from mmseg.models.utils.dacs_transforms import (denorm, get_class_masks,\n",
    "                                                get_mean_std, strong_transform)\n",
    "from mmseg.models.utils.visualization import subplotimg\n",
    "from mmseg.utils.utils import downscale_label_ratio\n",
    "\n",
    "from mmseg.utils import get_root_logger\n",
    "\n",
    "\n",
    "def _params_equal(ema_model, model):\n",
    "    for ema_param, param in zip(ema_model.named_parameters(),\n",
    "                                model.named_parameters()):\n",
    "        if not torch.equal(ema_param[1].data, param[1].data):\n",
    "            # print(\"Difference in\", ema_param[0])\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def calc_grad_magnitude(grads, norm_type=2.0):\n",
    "    norm_type = float(norm_type)\n",
    "    if norm_type == math.inf:\n",
    "        norm = max(p.abs().max() for p in grads)\n",
    "    else:\n",
    "        norm = torch.norm(\n",
    "            torch.stack([torch.norm(p, norm_type) for p in grads]), norm_type)\n",
    "\n",
    "    return norm\n",
    "def choose_with_probability(prob_true):\n",
    "    \"\"\"Chooses between True and False based on the given probability.\n",
    "\n",
    "    Args:\n",
    "        prob_true: The probability of returning True.\n",
    "\n",
    "    Returns:\n",
    "        True with the specified probability, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    if random.random() < prob_true:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "@UDA.register_module()\n",
    "class SMDACS(UDADecorator):\n",
    "\n",
    "    def __init__(self, **cfg):\n",
    "        super(SMDACS, self).__init__(**cfg)\n",
    "        self.local_iter = 0\n",
    "        self.max_iters = cfg['max_iters']\n",
    "        self.alpha = cfg['alpha']\n",
    "        self.pseudo_threshold = cfg['pseudo_threshold']\n",
    "        self.psweight_ignore_top = cfg['pseudo_weight_ignore_top']\n",
    "        self.psweight_ignore_bottom = cfg['pseudo_weight_ignore_bottom']\n",
    "        self.fdist_lambda = cfg['imnet_feature_dist_lambda']\n",
    "        self.fdist_classes = cfg['imnet_feature_dist_classes']\n",
    "        self.fdist_scale_min_ratio = cfg['imnet_feature_dist_scale_min_ratio']\n",
    "        self.enable_fdist = self.fdist_lambda > 0\n",
    "        self.mix = cfg['mix']\n",
    "        self.blur = cfg['blur']\n",
    "        self.color_jitter_s = cfg['color_jitter_strength']\n",
    "        self.color_jitter_p = cfg['color_jitter_probability']\n",
    "        self.debug_img_interval = cfg['debug_img_interval']\n",
    "        self.sm_prob = cfg['sm_prob']\n",
    "        self.debug_gt=0\n",
    "        self.debug_gt_interval = cfg['debug_gt_interval']\n",
    "        self.print_grad_magnitude = cfg['print_grad_magnitude']\n",
    "        self.cmap = cfg['cmap']\n",
    "        assert self.mix == 'class'\n",
    "\n",
    "        self.debug_fdist_mask = None\n",
    "        self.debug_gt_rescale = None\n",
    "\n",
    "        self.class_probs = {}\n",
    "        ema_cfg = deepcopy(cfg['model'])\n",
    "        self.ema_model = build_segmentor(ema_cfg)\n",
    "\n",
    "        if self.enable_fdist:\n",
    "            self.imnet_model = build_segmentor(deepcopy(cfg['model']))\n",
    "        else:\n",
    "            self.imnet_model = None\n",
    "\n",
    "    def get_ema_model(self):\n",
    "        return get_module(self.ema_model)\n",
    "\n",
    "    def get_imnet_model(self):\n",
    "        return get_module(self.imnet_model)\n",
    "\n",
    "    def _init_ema_weights(self):\n",
    "        for param in self.get_ema_model().parameters():\n",
    "            param.detach_()\n",
    "        mp = list(self.get_model().parameters())\n",
    "        mcp = list(self.get_ema_model().parameters())\n",
    "        for i in range(0, len(mp)):\n",
    "            if not mcp[i].data.shape:  # scalar tensor\n",
    "                mcp[i].data = mp[i].data.clone()\n",
    "            else:\n",
    "                mcp[i].data[:] = mp[i].data[:].clone()\n",
    "\n",
    "    def _update_ema(self, iter):\n",
    "        alpha_teacher = min(1 - 1 / (iter + 1), self.alpha)\n",
    "        for ema_param, param in zip(self.get_ema_model().parameters(),\n",
    "                                    self.get_model().parameters()):\n",
    "            if not param.data.shape:  # scalar tensor\n",
    "                ema_param.data = \\\n",
    "                    alpha_teacher * ema_param.data + \\\n",
    "                    (1 - alpha_teacher) * param.data\n",
    "            else:\n",
    "                ema_param.data[:] = \\\n",
    "                    alpha_teacher * ema_param[:].data[:] + \\\n",
    "                    (1 - alpha_teacher) * param[:].data[:]\n",
    "\n",
    "    def train_step(self, data_batch, optimizer, **kwargs):\n",
    "        \"\"\"The iteration step during training.\n",
    "\n",
    "        This method defines an iteration step during training, except for the\n",
    "        back propagation and optimizer updating, which are done in an optimizer\n",
    "        hook. Note that in some complicated cases or models, the whole process\n",
    "        including back propagation and optimizer updating is also defined in\n",
    "        this method, such as GAN.\n",
    "\n",
    "        Args:\n",
    "            data (dict): The output of dataloader.\n",
    "            optimizer (:obj:`torch.optim.Optimizer` | dict): The optimizer of\n",
    "                runner is passed to ``train_step()``. This argument is unused\n",
    "                and reserved.\n",
    "\n",
    "        Returns:\n",
    "            dict: It should contain at least 3 keys: ``loss``, ``log_vars``,\n",
    "                ``num_samples``.\n",
    "                ``loss`` is a tensor for back propagation, which can be a\n",
    "                weighted sum of multiple losses.\n",
    "                ``log_vars`` contains all the variables to be sent to the\n",
    "                logger.\n",
    "                ``num_samples`` indicates the batch size (when the model is\n",
    "                DDP, it means the batch size on each GPU), which is used for\n",
    "                averaging the logs.\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        log_vars = self(**data_batch)\n",
    "        optimizer.step()\n",
    "\n",
    "        log_vars.pop('loss', None)  # remove the unnecessary 'loss'\n",
    "        outputs = dict(\n",
    "            log_vars=log_vars, num_samples=len(data_batch['img_metas']))\n",
    "        return outputs\n",
    "\n",
    "    def masked_feat_dist(self, f1, f2, mask=None):\n",
    "        feat_diff = f1 - f2\n",
    "        # mmcv.print_log(f'fdiff: {feat_diff.shape}', 'mmseg')\n",
    "        pw_feat_dist = torch.norm(feat_diff, dim=1, p=2)\n",
    "        # mmcv.print_log(f'pw_fdist: {pw_feat_dist.shape}', 'mmseg')\n",
    "        if mask is not None:\n",
    "            # mmcv.print_log(f'fd mask: {mask.shape}', 'mmseg')\n",
    "            pw_feat_dist = pw_feat_dist[mask.squeeze(1)]\n",
    "            # mmcv.print_log(f'fd masked: {pw_feat_dist.shape}', 'mmseg')\n",
    "        return torch.mean(pw_feat_dist)\n",
    "\n",
    "    def calc_feat_dist(self, img, gt, feat=None):\n",
    "        assert self.enable_fdist\n",
    "        with torch.no_grad():\n",
    "            self.get_imnet_model().eval()\n",
    "            feat_imnet = self.get_imnet_model().extract_feat(img)\n",
    "            feat_imnet = [f.detach() for f in feat_imnet]\n",
    "        lay = -1\n",
    "        if self.fdist_classes is not None:\n",
    "            fdclasses = torch.tensor(self.fdist_classes, device=gt.device)\n",
    "            scale_factor = gt.shape[-1] // feat[lay].shape[-1]\n",
    "            gt_rescaled = downscale_label_ratio(gt, scale_factor,\n",
    "                                                self.fdist_scale_min_ratio,\n",
    "                                                self.num_classes,\n",
    "                                                255).long().detach()\n",
    "            fdist_mask = torch.any(gt_rescaled[..., None] == fdclasses, -1)\n",
    "            feat_dist = self.masked_feat_dist(feat[lay], feat_imnet[lay],\n",
    "                                              fdist_mask)\n",
    "            self.debug_fdist_mask = fdist_mask\n",
    "            self.debug_gt_rescale = gt_rescaled\n",
    "        else:\n",
    "            feat_dist = self.masked_feat_dist(feat[lay], feat_imnet[lay])\n",
    "        feat_dist = self.fdist_lambda * feat_dist\n",
    "        feat_loss, feat_log = self._parse_losses(\n",
    "            {'loss_imnet_feat_dist': feat_dist})\n",
    "        feat_log.pop('loss', None)\n",
    "        return feat_loss, feat_log\n",
    "\n",
    "    def forward_train(self, img, img_metas, gt_semantic_seg, target_img,\n",
    "                      target_img_metas, target_gt_semantic_seg, **kwargs):\n",
    "        \"\"\"Forward function for training.\n",
    "\n",
    "        Args:\n",
    "            img (Tensor): Input images.\n",
    "            img_metas (list[dict]): List of image info dict where each dict\n",
    "                has: 'img_shape', 'scale_factor', 'flip', and may also contain\n",
    "                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n",
    "                For details on the values of these keys see\n",
    "                `mmseg/datasets/pipelines/formatting.py:Collect`.\n",
    "            gt_semantic_seg (Tensor): Semantic segmentation masks\n",
    "                used if the architecture supports semantic segmentation task.\n",
    "\n",
    "        Returns:\n",
    "            dict[str, Tensor]: a dictionary of loss components\n",
    "        \"\"\"\n",
    "        log_vars = {}\n",
    "        batch_size = img.shape[0]\n",
    "        dev = img.device\n",
    "        # mmcv.print_log(f'{target_img_metas}', 'mmseg')\n",
    "        # Init/update ema model\n",
    "        if self.local_iter == 0:\n",
    "            self._init_ema_weights()\n",
    "            # assert _params_equal(self.get_ema_model(), self.get_model())\n",
    "\n",
    "        if self.local_iter > 0:\n",
    "            self._update_ema(self.local_iter)\n",
    "            # assert not _params_equal(self.get_ema_model(), self.get_model())\n",
    "            # assert self.get_ema_model().training\n",
    "\n",
    "        means, stds = get_mean_std(img_metas, dev)\n",
    "        strong_parameters = {\n",
    "            'mix': None,\n",
    "            'color_jitter': random.uniform(0, 1),\n",
    "            'color_jitter_s': self.color_jitter_s,\n",
    "            'color_jitter_p': self.color_jitter_p,\n",
    "            'blur': random.uniform(0, 1) if self.blur else 0,\n",
    "            'mean': means[0].unsqueeze(0),  # assume same normalization\n",
    "            'std': stds[0].unsqueeze(0)\n",
    "        }\n",
    "\n",
    "        # Train on source images\n",
    "        clean_losses = self.get_model().forward_train(\n",
    "            img, img_metas, gt_semantic_seg, return_feat=True)\n",
    "        src_feat = clean_losses.pop('features')\n",
    "        clean_loss, clean_log_vars = self._parse_losses(clean_losses)\n",
    "        log_vars.update(clean_log_vars)\n",
    "        clean_loss.backward(retain_graph=self.enable_fdist)\n",
    "        if self.print_grad_magnitude:\n",
    "            params = self.get_model().backbone.parameters()\n",
    "            seg_grads = [\n",
    "                p.grad.detach().clone() for p in params if p.grad is not None\n",
    "            ]\n",
    "            grad_mag = calc_grad_magnitude(seg_grads)\n",
    "            mmcv.print_log(f'Seg. Grad.: {grad_mag}', 'mmseg')\n",
    "\n",
    "        # ImageNet feature distance\n",
    "        if self.enable_fdist:\n",
    "            feat_loss, feat_log = self.calc_feat_dist(img, gt_semantic_seg,\n",
    "                                                      src_feat)\n",
    "            feat_loss.backward()\n",
    "            log_vars.update(add_prefix(feat_log, 'src'))\n",
    "            if self.print_grad_magnitude:\n",
    "                params = self.get_model().backbone.parameters()\n",
    "                fd_grads = [\n",
    "                    p.grad.detach() for p in params if p.grad is not None\n",
    "                ]\n",
    "                fd_grads = [g2 - g1 for g1, g2 in zip(seg_grads, fd_grads)]\n",
    "                grad_mag = calc_grad_magnitude(fd_grads)\n",
    "                mmcv.print_log(f'Fdist Grad.: {grad_mag}', 'mmseg')\n",
    "\n",
    "        # Generate pseudo-label\n",
    "        for m in self.get_ema_model().modules():\n",
    "            if isinstance(m, _DropoutNd):\n",
    "                m.training = False\n",
    "            if isinstance(m, DropPath):\n",
    "                m.training = False\n",
    "        ema_logits = self.get_ema_model().encode_decode(\n",
    "            target_img, target_img_metas)\n",
    "\n",
    "        ema_softmax = torch.softmax(ema_logits.detach(), dim=1)\n",
    "        pseudo_prob, pseudo_label = torch.max(ema_softmax, dim=1)\n",
    "        ps_large_p = pseudo_prob.ge(self.pseudo_threshold).long() == 1\n",
    "        ps_size = np.size(np.array(pseudo_label.cpu()))\n",
    "        pseudo_weight = torch.sum(ps_large_p).item() / ps_size\n",
    "        pseudo_weight = pseudo_weight * torch.ones(\n",
    "            pseudo_prob.shape, device=dev)\n",
    "\n",
    "        if self.psweight_ignore_top > 0:\n",
    "            # Don't trust pseudo-labels in regions with potential\n",
    "            # rectification artifacts. This can lead to a pseudo-label\n",
    "            # drift from sky towards building or traffic light.\n",
    "            pseudo_weight[:, :self.psweight_ignore_top, :] = 0\n",
    "        if self.psweight_ignore_bottom > 0:\n",
    "            pseudo_weight[:, -self.psweight_ignore_bottom:, :] = 0\n",
    "\n",
    "        # mmcv.print_log(target_img_metas , logger=get_root_logger())\n",
    "        use_gt = []\n",
    "        for i in range(batch_size):\n",
    "            # mmcv.print_log(target_img_metas[i].get('filename').split('.')[0], logger=get_root_logger())\n",
    "            # use_gt.append(target_img_metas[i].get('use_gt',False))  \n",
    "            # use_gt_ = choose_with_probability(self.sm_prob)   \n",
    "                  \n",
    "            use_gt.append(target_img_metas[i].get('with_labels', False))\n",
    "        \n",
    "            # mmcv.print_log(target_img_metas[i].get('ori_filename').split('.')[0], logger=get_root_logger())\n",
    "            if target_img_metas[i].get('with_labels', False) :\n",
    "                # pseudo_weight[i] = torch.ones((pseudo_weight[i].shape), device=dev)\n",
    "                pseudo_label[i] = target_gt_semantic_seg[i]\n",
    "                self.debug_gt +=1\n",
    "        # mmcv.print_log(use_gt, logger=get_root_logger())\n",
    "\n",
    "        gt_pixel_weight = torch.ones((pseudo_weight.shape), device=dev)\n",
    "\n",
    "        # Apply mixing\n",
    "        mixed_img, mixed_lbl = [None] * batch_size, [None] * batch_size\n",
    "        mix_masks = get_class_masks(gt_semantic_seg)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            strong_parameters['mix'] = mix_masks[i]\n",
    "            mixed_img[i], mixed_lbl[i] = strong_transform(\n",
    "                strong_parameters,\n",
    "                data=torch.stack((img[i], target_img[i])),\n",
    "                target=torch.stack((gt_semantic_seg[i][0], pseudo_label[i])))\n",
    "            _, pseudo_weight[i] = strong_transform(\n",
    "                strong_parameters,\n",
    "                target=torch.stack((gt_pixel_weight[i], pseudo_weight[i])))\n",
    "        mixed_img = torch.cat(mixed_img)\n",
    "        mixed_lbl = torch.cat(mixed_lbl)\n",
    "\n",
    "        # Train on mixed images\n",
    "        mix_losses = self.get_model().forward_train(\n",
    "            mixed_img, img_metas, mixed_lbl, pseudo_weight, return_feat=True)\n",
    "        mix_losses.pop('features')\n",
    "        mix_losses = add_prefix(mix_losses, 'mix')\n",
    "        mix_loss, mix_log_vars = self._parse_losses(mix_losses)\n",
    "        log_vars.update(mix_log_vars)\n",
    "        mix_loss.backward()\n",
    "        # for i in range(batch_size):\n",
    "            # unique_values= torch.unique(gt_semantic_seg[i], return_counts=False)\n",
    "            # mmcv.print_log(f\"Source: {unique_values}\", logger=get_root_logger())\n",
    "                    \n",
    "            # unique_values = torch.unique(target_gt_semantic_seg[i], return_counts=False)\n",
    "            # mmcv.print_log(f\"Target: {unique_values}\", logger=get_root_logger())\n",
    "            # mmcv.print_log(gt_semantic_seg[i], logger=get_root_logger())\n",
    "        if self.local_iter % self.debug_img_interval == 0:\n",
    "            out_dir = os.path.join(self.train_cfg['work_dir'],\n",
    "                                   'class_mix_debug')\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            vis_img = torch.clamp(denorm(img, means, stds), 0, 1)\n",
    "            vis_trg_img = torch.clamp(denorm(target_img, means, stds), 0, 1)\n",
    "            vis_mixed_img = torch.clamp(denorm(mixed_img, means, stds), 0, 1)           \n",
    "            \n",
    "            for j in range(batch_size):\n",
    "                rows, cols = 2, 5\n",
    "                fig, axs = plt.subplots(\n",
    "                    rows,\n",
    "                    cols,\n",
    "                    figsize=(3 * cols, 3 * rows),\n",
    "                    gridspec_kw={\n",
    "                        'hspace': 0.1,\n",
    "                        'wspace': 0,\n",
    "                        'top': 0.95,\n",
    "                        'bottom': 0,\n",
    "                        'right': 1,\n",
    "                        'left': 0\n",
    "                    },\n",
    "                )\n",
    "                subplotimg(axs[0][0], vis_img[j], 'Source Image')\n",
    "                subplotimg(axs[1][0], vis_trg_img[j], 'Target Image')\n",
    "                subplotimg(\n",
    "                    axs[0][1],\n",
    "                    gt_semantic_seg[j],\n",
    "                    'Source Seg GT',\n",
    "                    cmap=self.cmap)\n",
    "                subplotimg(\n",
    "                    axs[1][1],\n",
    "                    target_gt_semantic_seg[j],\n",
    "                    'Target Seg GT',\n",
    "                    cmap=self.cmap)\n",
    "                \n",
    "                if use_gt[j] is True:\n",
    "                    subplotimg(\n",
    "                        axs[0][2],\n",
    "                        pseudo_label[j],\n",
    "                        'Target Seg (Mix) GT',\n",
    "                    cmap=self.cmap)\n",
    "                else:\n",
    "                    subplotimg(\n",
    "                        axs[0][2],\n",
    "                        pseudo_label[j],\n",
    "                        'Target Seg (Pseudo) GT',\n",
    "                    cmap=self.cmap)\n",
    "\n",
    "                \n",
    "                subplotimg(axs[0][3], vis_mixed_img[j], 'Mixed Image')\n",
    "                subplotimg(\n",
    "                    axs[1][3], mix_masks[j][0], 'Domain Mask', cmap='gray')\n",
    "                # subplotimg(axs[0][3], pred_u_s[j], \"Seg Pred\",\n",
    "                #            cmap=\"cityscapes\")\n",
    "                subplotimg(\n",
    "                    axs[0][4], mixed_lbl[j], 'Seg Targ', cmap=self.cmap)\n",
    "                subplotimg(\n",
    "                    axs[1][4], pseudo_weight[j], 'Pseudo W.', vmin=0, vmax=1)\n",
    "                if self.debug_fdist_mask is not None:\n",
    "                    subplotimg(\n",
    "                        axs[0][5],\n",
    "                        self.debug_fdist_mask[j][0],\n",
    "                        'FDist Mask',\n",
    "                        cmap='gray')\n",
    "                if self.debug_gt_rescale is not None:\n",
    "                    subplotimg(\n",
    "                        axs[1][5],\n",
    "                        self.debug_gt_rescale[j],\n",
    "                        'Scaled GT',\n",
    "                        cmap=self.cmap)\n",
    "                for ax in axs.flat:\n",
    "                    ax.axis('off')\n",
    "                plt.savefig(\n",
    "                    os.path.join(out_dir,\n",
    "                                 f'{(self.local_iter + 1):06d}_{j}.png'))\n",
    "                plt.close()\n",
    "        self.local_iter += 1\n",
    "        use_gt.clear()\n",
    "\n",
    "        return log_vars\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
